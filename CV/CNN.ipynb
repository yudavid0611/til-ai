{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(Convolutional Neural Network)\n",
    "> 참고: 책 <<밑바닥부터 시작하는 딥러닝1>>, pp.242-253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from common.util import im2col, col2im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # filter shape: (출력 채널 수, 입력 채널 수, 높이, 너비)\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "\n",
    "        # input data shape: (데이터 수, 채널 수, 높이, 너비)\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # 출력(feature map)의 높이와 너비\n",
    "        out_h = int(1 + (H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2 * self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        # 필터 전개\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        # transpose로 축의 순서를 바꿔준다.\n",
    "        # (N, H, W, C) -> (N, C, H, W)\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 전개\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        \n",
    "        # 최대값\n",
    "        out = np.max(col, axis=1)\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from common.layers import Relu, Affine, SoftmaxWithLoss\n",
    "# from common.gradient import numerical_gradient\n",
    "# import pickle\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={\n",
    "                     'filter_num': 30,\n",
    "                     'filter_size': 5,\n",
    "                     'pad': 0,\n",
    "                     'stride': 1\n",
    "                 }, hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "\n",
    "        # 합성곱 계층의 출력 크기\n",
    "        conv_output_size = (input_size - filter_size + 2 * filter_pad) / filter_stride + 1\n",
    "\n",
    "        # 풀링 계층의 출력 크기\n",
    "        pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\n",
    "        \n",
    "        # 가중치 파라미터\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 레이어\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], filter_stride, filter_pad)\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    \n",
    "    # 추론\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        layer에 추가한 계층을 맨 앞부터 차례대로 forward를 호출하여 \n",
    "        결과를 다음 계층에 전달\n",
    "        '''\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    # 손실 함수 값 구하기\n",
    "    def loss(self, x, t):\n",
    "        '''\n",
    "        x: 입력 데이터\n",
    "        t: 정답 레이블\n",
    "        '''\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "\n",
    "    # 오차역전파\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 기울기 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "\n",
    "    # def accuracy(self, x, t, batch_size=100):\n",
    "    #     if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "    #     acc = 0.0\n",
    "        \n",
    "    #     for i in range(int(x.shape[0] / batch_size)):\n",
    "    #         tx = x[i*batch_size:(i+1)*batch_size]\n",
    "    #         tt = t[i*batch_size:(i+1)*batch_size]\n",
    "    #         y = self.predict(tx)\n",
    "    #         y = np.argmax(y, axis=1)\n",
    "    #         acc += np.sum(y == tt) \n",
    "        \n",
    "    #     return acc / x.shape[0]\n",
    "\n",
    "\n",
    "    # def numerical_gradient(self, x, t):\n",
    "    #     loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "    #     grads = {}\n",
    "    #     for idx in (1, 2, 3):\n",
    "    #         grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "    #         grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "    #     return grads\n",
    "\n",
    "    # def save_params(self, file_name=\"params.pkl\"):\n",
    "    #     params = {}\n",
    "    #     for key, val in self.params.items():\n",
    "    #         params[key] = val\n",
    "    #     with open(file_name, 'wb') as f:\n",
    "    #         pickle.dump(params, f)\n",
    "\n",
    "    # def load_params(self, file_name=\"params.pkl\"):\n",
    "    #     with open(file_name, 'rb') as f:\n",
    "    #         params = pickle.load(f)\n",
    "    #     for key, val in params.items():\n",
    "    #         self.params[key] = val\n",
    "\n",
    "    #     for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "    #         self.layers[key].W = self.params['W' + str(i+1)]\n",
    "    #         self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2997761366472145\n",
      "=== epoch:1, train acc:0.148, test acc:0.133 ===\n",
      "train loss:2.2975034272558785\n",
      "train loss:2.2937057915710817\n",
      "train loss:2.290231301500161\n",
      "train loss:2.2815961106491542\n",
      "train loss:2.273117443613491\n",
      "train loss:2.263357824373143\n",
      "train loss:2.2411198416729623\n",
      "train loss:2.196539184898299\n",
      "train loss:2.217913109094189\n",
      "train loss:2.156688716724658\n",
      "train loss:2.1278800196706875\n",
      "train loss:2.1264475511616414\n",
      "train loss:2.07589388202547\n",
      "train loss:2.0492835513325427\n",
      "train loss:1.9982507714442659\n",
      "train loss:1.8841290708867904\n",
      "train loss:1.8364878752091673\n",
      "train loss:1.7988773981255881\n",
      "train loss:1.6480267796233738\n",
      "train loss:1.6729247191420327\n",
      "train loss:1.586668043927189\n",
      "train loss:1.4857741646497653\n",
      "train loss:1.38189865364824\n",
      "train loss:1.3110422674335487\n",
      "train loss:1.2072953992670046\n",
      "train loss:1.2173083037894916\n",
      "train loss:0.9998792683509599\n",
      "train loss:1.06269297249489\n",
      "train loss:1.034266608838847\n",
      "train loss:0.8884760679651187\n",
      "train loss:0.9890882668636257\n",
      "train loss:0.755776244823171\n",
      "train loss:1.0166758778079248\n",
      "train loss:0.7380810741304994\n",
      "train loss:0.7861335468054734\n",
      "train loss:0.6392502917746854\n",
      "train loss:0.8357493032946866\n",
      "train loss:0.9253736975730376\n",
      "train loss:0.921211605083018\n",
      "train loss:0.7033510665288101\n",
      "train loss:0.6256167025837214\n",
      "train loss:0.5766722011672968\n",
      "train loss:0.6092004398113319\n",
      "train loss:0.566360009531268\n",
      "train loss:0.6028773645559907\n",
      "train loss:0.44383765179107415\n",
      "train loss:0.6000532422282199\n",
      "train loss:0.5304899013951598\n",
      "train loss:0.5905486860987496\n",
      "train loss:0.35762762755872146\n",
      "train loss:0.5474378937917646\n",
      "train loss:0.5892342362337056\n",
      "train loss:0.5104219104718655\n",
      "train loss:0.5163435582355873\n",
      "train loss:0.40761570211883863\n",
      "train loss:0.36166795990219547\n",
      "train loss:0.31857535017348915\n",
      "train loss:0.5078734824283975\n",
      "train loss:0.48105777458710103\n",
      "train loss:0.4182099728199126\n",
      "train loss:0.5003780922599883\n",
      "train loss:0.512225699516922\n",
      "train loss:0.4581652507974514\n",
      "train loss:0.3226320794331653\n",
      "train loss:0.5322468380372393\n",
      "train loss:0.3738123859011825\n",
      "train loss:0.46766281169473883\n",
      "train loss:0.6823805986243676\n",
      "train loss:0.3391348575164234\n",
      "train loss:0.5554972976588614\n",
      "train loss:0.3276954780522483\n",
      "train loss:0.4294562451504035\n",
      "train loss:0.48221864556348903\n",
      "train loss:0.4574089341659811\n",
      "train loss:0.4444854552432187\n",
      "train loss:0.3913408931379714\n",
      "train loss:0.3647281417298067\n",
      "train loss:0.3723623942912664\n",
      "train loss:0.4860054554634028\n",
      "train loss:0.42587758077341226\n",
      "train loss:0.3208602601002815\n",
      "train loss:0.4146524584547218\n",
      "train loss:0.32750697226926706\n",
      "train loss:0.3238857415591051\n",
      "train loss:0.29120467132962835\n",
      "train loss:0.4767647046696326\n",
      "train loss:0.27556816997838324\n",
      "train loss:0.510001330949681\n",
      "train loss:0.5065154419970703\n",
      "train loss:0.3080977513494071\n",
      "train loss:0.47213669362532296\n",
      "train loss:0.49410554055966643\n",
      "train loss:0.39483240888242077\n",
      "train loss:0.5841421552234961\n",
      "train loss:0.4548100168429729\n",
      "train loss:0.6599252404960555\n",
      "train loss:0.44435098782022386\n",
      "train loss:0.33381154798333035\n",
      "train loss:0.5819286113334137\n",
      "train loss:0.40526404852347125\n",
      "train loss:0.3430704937359592\n",
      "train loss:0.32344877187228266\n",
      "train loss:0.5180319874939484\n",
      "train loss:0.329122340412322\n",
      "train loss:0.46884445578112977\n",
      "train loss:0.32246941764459996\n",
      "train loss:0.3597334997862029\n",
      "train loss:0.40145620250994574\n",
      "train loss:0.4115971043156161\n",
      "train loss:0.41514148824071306\n",
      "train loss:0.555939782532189\n",
      "train loss:0.3707295175195898\n",
      "train loss:0.3036125513742225\n",
      "train loss:0.6029843373606433\n",
      "train loss:0.4742440224754409\n",
      "train loss:0.29515870510294473\n",
      "train loss:0.3611675674543681\n",
      "train loss:0.40201040365975016\n",
      "train loss:0.28606197073557915\n",
      "train loss:0.23849107435809513\n",
      "train loss:0.3667969449881761\n",
      "train loss:0.2635285245118302\n",
      "train loss:0.26910496352695873\n",
      "train loss:0.28716031439044254\n",
      "train loss:0.4296145549609963\n",
      "train loss:0.5557002793683149\n",
      "train loss:0.30788561637903616\n",
      "train loss:0.23388004395275755\n",
      "train loss:0.40577940333438\n",
      "train loss:0.4640812968892003\n",
      "train loss:0.2608455717016794\n",
      "train loss:0.35338705748311633\n",
      "train loss:0.2888863918425525\n",
      "train loss:0.29647844855048594\n",
      "train loss:0.4273897514962204\n",
      "train loss:0.3019861636673566\n",
      "train loss:0.2918837862138668\n",
      "train loss:0.3168763621694636\n",
      "train loss:0.34407830426903274\n",
      "train loss:0.2849181325134695\n",
      "train loss:0.29719244175958853\n",
      "train loss:0.3279117983313148\n",
      "train loss:0.45144282900689936\n",
      "train loss:0.39454805301185014\n",
      "train loss:0.3665813757792408\n",
      "train loss:0.273116494918555\n",
      "train loss:0.4030200502954859\n",
      "train loss:0.2726595688882305\n",
      "train loss:0.5217853281903649\n",
      "train loss:0.17608622241606195\n",
      "train loss:0.4141554768106868\n",
      "train loss:0.37161136337091777\n",
      "train loss:0.16177513802435897\n",
      "train loss:0.3951872900008794\n",
      "train loss:0.14601709124288498\n",
      "train loss:0.5302228913683522\n",
      "train loss:0.26131588147780804\n",
      "train loss:0.38043882066235335\n",
      "train loss:0.24769086139742771\n",
      "train loss:0.39896410419436473\n",
      "train loss:0.31173794749743033\n",
      "train loss:0.23409567182332627\n",
      "train loss:0.35117039270679967\n",
      "train loss:0.27620215738957177\n",
      "train loss:0.37187315239032215\n",
      "train loss:0.33737029569718646\n",
      "train loss:0.2798293681646997\n",
      "train loss:0.17617713269921734\n",
      "train loss:0.2222061245676256\n",
      "train loss:0.411532872044162\n",
      "train loss:0.35638960951389903\n",
      "train loss:0.41660274033299893\n",
      "train loss:0.20004299949826979\n",
      "train loss:0.2330768357895147\n",
      "train loss:0.4350168401948804\n",
      "train loss:0.1937493897296151\n",
      "train loss:0.13112682647980545\n",
      "train loss:0.3738526922566138\n",
      "train loss:0.2990569170472931\n",
      "train loss:0.3032006348060024\n",
      "train loss:0.4522762529833637\n",
      "train loss:0.250652630573166\n",
      "train loss:0.24875586960171112\n",
      "train loss:0.41746268305623924\n",
      "train loss:0.29130014063177784\n",
      "train loss:0.2467634708432173\n",
      "train loss:0.370932947818005\n",
      "train loss:0.18060913153029443\n",
      "train loss:0.34821623532503887\n",
      "train loss:0.24122071155224384\n",
      "train loss:0.2212926042103128\n",
      "train loss:0.1560965626561363\n",
      "train loss:0.19607996464130764\n",
      "train loss:0.33113269841735193\n",
      "train loss:0.19776371819320357\n",
      "train loss:0.5580812377185225\n",
      "train loss:0.22479409806409595\n",
      "train loss:0.3070809222009018\n",
      "train loss:0.21386011479757286\n",
      "train loss:0.3357591628969295\n",
      "train loss:0.3250425758904931\n",
      "train loss:0.26329403887593666\n",
      "train loss:0.2841820839864444\n",
      "train loss:0.32851103957755257\n",
      "train loss:0.24264061485062804\n",
      "train loss:0.3555974701835384\n",
      "train loss:0.4238621254621314\n",
      "train loss:0.33505776865770287\n",
      "train loss:0.3224769718667092\n",
      "train loss:0.3003263546896725\n",
      "train loss:0.277521799387703\n",
      "train loss:0.22967170257532163\n",
      "train loss:0.2574890181579089\n",
      "train loss:0.33213382386133355\n",
      "train loss:0.19850171904817862\n",
      "train loss:0.32809073264413685\n",
      "train loss:0.23637793687277575\n",
      "train loss:0.2735330629134576\n",
      "train loss:0.2456392730959396\n",
      "train loss:0.3127168121818667\n",
      "train loss:0.3065548719030453\n",
      "train loss:0.37948833072435784\n",
      "train loss:0.25211279460321917\n",
      "train loss:0.29401715028946124\n",
      "train loss:0.3071408300822068\n",
      "train loss:0.3215101448595105\n",
      "train loss:0.21274592564904096\n",
      "train loss:0.2096976024229809\n",
      "train loss:0.21227547676186515\n",
      "train loss:0.3908402475803067\n",
      "train loss:0.2768655189448213\n",
      "train loss:0.2781089119821514\n",
      "train loss:0.22624449614420417\n",
      "train loss:0.2738231796386159\n",
      "train loss:0.2519817502227916\n",
      "train loss:0.2387117689147921\n",
      "train loss:0.2732553620284712\n",
      "train loss:0.27555896098455235\n",
      "train loss:0.33343116634677\n",
      "train loss:0.24951899182149997\n",
      "train loss:0.345705499564952\n",
      "train loss:0.3611159761888208\n",
      "train loss:0.2367164858691425\n",
      "train loss:0.36922332784230205\n",
      "train loss:0.19733178779389443\n",
      "train loss:0.2969097946150258\n",
      "train loss:0.2993366787624371\n",
      "train loss:0.193166852631171\n",
      "train loss:0.21613232251714978\n",
      "train loss:0.3697456129314222\n",
      "train loss:0.27968732279263775\n",
      "train loss:0.3736239456507806\n",
      "train loss:0.24937968556912957\n",
      "train loss:0.37849049786060435\n",
      "train loss:0.22288089170363104\n",
      "train loss:0.3433196801420402\n",
      "train loss:0.2750326747795496\n",
      "train loss:0.3477975398221148\n",
      "train loss:0.29063233452137527\n",
      "train loss:0.2374181145044528\n",
      "train loss:0.26002531921030675\n",
      "train loss:0.23184180434823604\n",
      "train loss:0.22121128664611395\n",
      "train loss:0.3415953318175929\n",
      "train loss:0.19476311228695484\n",
      "train loss:0.248147658059991\n",
      "train loss:0.2514896450904562\n",
      "train loss:0.30923902134081227\n",
      "train loss:0.1616874351467076\n",
      "train loss:0.3183167967334755\n",
      "train loss:0.3069906752488592\n",
      "train loss:0.32709677457397635\n",
      "train loss:0.2355107960836347\n",
      "train loss:0.27441253640738283\n",
      "train loss:0.2646344207643393\n",
      "train loss:0.19873625022101574\n",
      "train loss:0.25435708332314727\n",
      "train loss:0.14805175411688237\n",
      "train loss:0.2948922314330704\n",
      "train loss:0.285311189384115\n",
      "train loss:0.22187880770655422\n",
      "train loss:0.30532175030260583\n",
      "train loss:0.22763651143693844\n",
      "train loss:0.14891933474422714\n",
      "train loss:0.1836516548723656\n",
      "train loss:0.25274244479768854\n",
      "train loss:0.21230946913062224\n",
      "train loss:0.21317955298158442\n",
      "train loss:0.23546633898789246\n",
      "train loss:0.29991008226778076\n",
      "train loss:0.18562461475161446\n",
      "train loss:0.4146500658143693\n",
      "train loss:0.19721047205770667\n",
      "train loss:0.22537442846564149\n",
      "train loss:0.24183661143854118\n",
      "train loss:0.3806951472189237\n",
      "train loss:0.25776210673837907\n",
      "train loss:0.11912653890102645\n",
      "train loss:0.16702255539161304\n",
      "train loss:0.32667216514594194\n",
      "train loss:0.2578452918905382\n",
      "train loss:0.3063639171855856\n",
      "train loss:0.2786419735095268\n",
      "train loss:0.25159444629757166\n",
      "train loss:0.31361117490784385\n",
      "train loss:0.22097483605733523\n",
      "train loss:0.10199282429332822\n",
      "train loss:0.2766108786573929\n",
      "train loss:0.19295164084732142\n",
      "train loss:0.14631240354144434\n",
      "train loss:0.12790062946617786\n",
      "train loss:0.33986250496558795\n",
      "train loss:0.18477852335377445\n",
      "train loss:0.16044005455619362\n",
      "train loss:0.3707401263938783\n",
      "train loss:0.1544414917028069\n",
      "train loss:0.12624592852366376\n",
      "train loss:0.22017928443754187\n",
      "train loss:0.17899676531966793\n",
      "train loss:0.3220325172365299\n",
      "train loss:0.25874363758626534\n",
      "train loss:0.16751871507270177\n",
      "train loss:0.13033523182099357\n",
      "train loss:0.33099916763999204\n",
      "train loss:0.15931946526787558\n",
      "train loss:0.25685892326905646\n",
      "train loss:0.28308502601426944\n",
      "train loss:0.14186230545659032\n",
      "train loss:0.2663749046834668\n",
      "train loss:0.18098986515160598\n",
      "train loss:0.22718719802897572\n",
      "train loss:0.28555595982662507\n",
      "train loss:0.175583794955242\n",
      "train loss:0.16327581700736768\n",
      "train loss:0.17665051672337925\n",
      "train loss:0.20112391515775294\n",
      "train loss:0.23349216721191585\n",
      "train loss:0.330870327819372\n",
      "train loss:0.1386977151023668\n",
      "train loss:0.10768267928775765\n",
      "train loss:0.2038223613879485\n",
      "train loss:0.20414911550210402\n",
      "train loss:0.1334449765472575\n",
      "train loss:0.17587420743895624\n",
      "train loss:0.32961849467203486\n",
      "train loss:0.19610588479580457\n",
      "train loss:0.14702126953270656\n",
      "train loss:0.19709951275621596\n",
      "train loss:0.32388691277693504\n",
      "train loss:0.2521168167491472\n",
      "train loss:0.15489970793074043\n",
      "train loss:0.10612218498025387\n",
      "train loss:0.1823254556031074\n",
      "train loss:0.15994201474032751\n",
      "train loss:0.20965799311862177\n",
      "train loss:0.16005436759336925\n",
      "train loss:0.15323534534635258\n",
      "train loss:0.20536766731385148\n",
      "train loss:0.24420393111783986\n",
      "train loss:0.09587353466879275\n",
      "train loss:0.11626173683564575\n",
      "train loss:0.27287664767789954\n",
      "train loss:0.2184713119822397\n",
      "train loss:0.237693446673038\n",
      "train loss:0.12398605921274729\n",
      "train loss:0.21198481895243998\n",
      "train loss:0.175928139590879\n",
      "train loss:0.2713133010950271\n",
      "train loss:0.2481299933230088\n",
      "train loss:0.12782766291607006\n",
      "train loss:0.3209883601178586\n",
      "train loss:0.16146668911078707\n",
      "train loss:0.19359157547523043\n",
      "train loss:0.19299423579738217\n",
      "train loss:0.08329540551147793\n",
      "train loss:0.14107500153641875\n",
      "train loss:0.24279754208346507\n",
      "train loss:0.21102178301904573\n",
      "train loss:0.21209623888448803\n",
      "train loss:0.31707858761837665\n",
      "train loss:0.20524014906938123\n",
      "train loss:0.1591783382276513\n",
      "train loss:0.26594876177121474\n",
      "train loss:0.10536028438341102\n",
      "train loss:0.23849558116023836\n",
      "train loss:0.16238582946203745\n",
      "train loss:0.165022954690376\n",
      "train loss:0.2604853289842886\n",
      "train loss:0.2100759323235007\n",
      "train loss:0.14414230984554577\n",
      "train loss:0.24746209330263036\n",
      "train loss:0.20873909537569119\n",
      "train loss:0.20568892365263902\n",
      "train loss:0.1710194686897213\n",
      "train loss:0.21718081467796793\n",
      "train loss:0.12262141544759253\n",
      "train loss:0.253814590853207\n",
      "train loss:0.21038181436999207\n",
      "train loss:0.18025649926899168\n",
      "train loss:0.19547752079690559\n",
      "train loss:0.09782885060003953\n",
      "train loss:0.23011473521577275\n",
      "train loss:0.15551383374369906\n",
      "train loss:0.14506570919731293\n",
      "train loss:0.1713369226294498\n",
      "train loss:0.1502559708951786\n",
      "train loss:0.14706031907537603\n",
      "train loss:0.24405038986740712\n",
      "train loss:0.25225577111690334\n",
      "train loss:0.18511134769070153\n",
      "train loss:0.1437404836191048\n",
      "train loss:0.1584708803630201\n",
      "train loss:0.08624389041548909\n",
      "train loss:0.09563583001986743\n",
      "train loss:0.14397968335657466\n",
      "train loss:0.19550218354108448\n",
      "train loss:0.12670210722037395\n",
      "train loss:0.2296542926587243\n",
      "train loss:0.1793664733954793\n",
      "train loss:0.1798707215266212\n",
      "train loss:0.22711572224992987\n",
      "train loss:0.2398070272201766\n",
      "train loss:0.17823286154236478\n",
      "train loss:0.13464378473615685\n",
      "train loss:0.14797447705531408\n",
      "train loss:0.26226963167272777\n",
      "train loss:0.20952052476031063\n",
      "train loss:0.20547751913180387\n",
      "train loss:0.1516951982001621\n",
      "train loss:0.1602388714439832\n",
      "train loss:0.07201423520965476\n",
      "train loss:0.22797106215943255\n",
      "train loss:0.17747074162286466\n",
      "train loss:0.13540568604393413\n",
      "train loss:0.19775345515698536\n",
      "train loss:0.15626220337800012\n",
      "train loss:0.1588711723502289\n",
      "train loss:0.3122718852973942\n",
      "train loss:0.13454054077918254\n",
      "train loss:0.17962728862715077\n",
      "train loss:0.15930016129679025\n",
      "train loss:0.23393281545194752\n",
      "train loss:0.15139982844586983\n",
      "train loss:0.1604694400246868\n",
      "train loss:0.313302544979694\n",
      "train loss:0.14663953033637164\n",
      "train loss:0.17229634113999576\n",
      "train loss:0.10856231911119994\n",
      "train loss:0.13891172004329688\n",
      "train loss:0.11228837772557716\n",
      "train loss:0.279857018217799\n",
      "train loss:0.16259662237412748\n",
      "train loss:0.09195133550598422\n",
      "train loss:0.14690384213678567\n",
      "train loss:0.1614979972415559\n",
      "train loss:0.136261705931277\n",
      "train loss:0.12274814308102196\n",
      "train loss:0.2293532273136144\n",
      "train loss:0.13654987779647154\n",
      "train loss:0.09298956612649845\n",
      "train loss:0.15725760831068772\n",
      "train loss:0.17101582013417915\n",
      "train loss:0.08033616967985276\n",
      "train loss:0.1518409333914794\n",
      "train loss:0.09068100938957604\n",
      "train loss:0.11731342921066715\n",
      "train loss:0.28517826708379035\n",
      "train loss:0.1557394557833206\n",
      "train loss:0.17269308632270403\n",
      "train loss:0.20169892156803976\n",
      "train loss:0.1891683036739401\n",
      "train loss:0.20915284736675854\n",
      "train loss:0.13027989448955313\n",
      "train loss:0.20193943182105795\n",
      "train loss:0.12530510805858128\n",
      "train loss:0.2583118304740143\n",
      "train loss:0.14856627851318582\n",
      "train loss:0.20962949894677677\n",
      "train loss:0.17933529715538735\n",
      "train loss:0.11350646571908767\n",
      "train loss:0.2846075901274759\n",
      "train loss:0.14395909705933763\n",
      "train loss:0.1490228339199845\n",
      "train loss:0.11933964049244164\n",
      "train loss:0.1925145407854447\n",
      "train loss:0.058173000198179003\n",
      "train loss:0.20762245566335746\n",
      "train loss:0.14024392156890025\n",
      "train loss:0.15764379901082384\n",
      "train loss:0.07598302671273353\n",
      "train loss:0.06485948926377899\n",
      "train loss:0.12118457043502621\n",
      "train loss:0.10947107975659909\n",
      "train loss:0.11042978124231725\n",
      "train loss:0.1081887768723211\n",
      "train loss:0.2751335290738277\n",
      "train loss:0.11041809292097705\n",
      "train loss:0.1984787722615986\n",
      "train loss:0.17130343597794784\n",
      "train loss:0.1480233149691863\n",
      "train loss:0.11769303229735371\n",
      "train loss:0.19400838369108223\n",
      "train loss:0.16810767801880322\n",
      "train loss:0.2747631257201564\n",
      "train loss:0.08953311575091348\n",
      "train loss:0.229967949561287\n",
      "train loss:0.061224774904581496\n",
      "train loss:0.16423130918165477\n",
      "train loss:0.20176641559074673\n",
      "train loss:0.1585221302580204\n",
      "train loss:0.143598887393179\n",
      "train loss:0.17190512969913319\n",
      "train loss:0.07602581734556677\n",
      "train loss:0.12785814652704594\n",
      "train loss:0.14162354708716868\n",
      "train loss:0.07992574834198883\n",
      "train loss:0.08652091741084025\n",
      "train loss:0.055847058315024595\n",
      "train loss:0.14661178048027182\n",
      "train loss:0.1264515495807868\n",
      "train loss:0.10061487871514803\n",
      "train loss:0.09427709643894466\n",
      "train loss:0.10496779823556071\n",
      "train loss:0.0950957642068096\n",
      "train loss:0.2058979623440572\n",
      "train loss:0.13489466988624463\n",
      "train loss:0.20239411384804273\n",
      "train loss:0.09231625318321356\n",
      "train loss:0.13758532301770646\n",
      "train loss:0.16446361470980222\n",
      "train loss:0.136212860908476\n",
      "train loss:0.13899182996835074\n",
      "train loss:0.09559348590846659\n",
      "train loss:0.13593908002430316\n",
      "train loss:0.1802928587942338\n",
      "train loss:0.18496572093564034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-263c512d405a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# 매개변수 보존\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\loven\\dev\\til-ml\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\loven\\dev\\til-ml\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-5f6841a410f5>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# 기울기 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a06cc444574d>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from datasets.mnist import load_mnist\n",
    "# from common.trainer import Trainer\n",
    "\n",
    "# # 데이터 읽기\n",
    "# (x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# # 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "# # x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "# # x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "# max_epochs = 20\n",
    "\n",
    "# network = SimpleConvNet(input_dim=(1,28,28), \n",
    "#                         conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "#                         hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "# trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "#                   epochs=max_epochs, mini_batch_size=100,\n",
    "#                   optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "#                   evaluate_sample_num_per_epoch=1000)\n",
    "# trainer.train()\n",
    "\n",
    "# # 매개변수 보존\n",
    "# network.save_params(\"params.pkl\")\n",
    "# print(\"Saved Network Parameters!\")\n",
    "\n",
    "# # 그래프 그리기\n",
    "# markers = {'train': 'o', 'test': 's'}\n",
    "# x = np.arange(max_epochs)\n",
    "# plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "# plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "# plt.xlabel(\"epochs\")\n",
    "# plt.ylabel(\"accuracy\")\n",
    "# plt.ylim(0, 1.0)\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
